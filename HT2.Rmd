---
title: "HT2"
author: "Stefan Quintana, Sofía Escobar, Wilfredo Gallegos"
date: "2/16/2023"
output: html_document
---

```{r, echo=FALSE}
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(dplyr)
library(ggplot2)
```

## PROBLEMA 1

```{r setup, include=TRUE, cache = FALSE}
datos<-read.csv("movies.csv")
set.seed(123)
datos<-datos[complete.cases(datos),]
summary(datos)
```

### budget 
```{r, echo = FALSE}
library(nortest)
qqnorm(datos$budget)
qqline(datos$budget)
lillie.test(datos$budget)
```
### revenue
```{r, echo = FALSE}
qqnorm(datos$budget)
qqline(datos$budget)
lillie.test(datos$revenue)
```
Basándose en la gráfica, donde se puede notar que los puntos se alejan de la recta teórica, podemos argumentar que los datos no respetan una distribución normal, lo cuál se confirma en el test Kolmogorov-Smirnov, donde se rechaza la hipótesis nula, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### runtime 
```{r, echo = FALSE}
qqnorm(datos$runtime)
qqline(datos$runtime)
lillie.test(datos$runtime)
```
Análogamente con el set de datos anterior, el valor p del test Kolmogorov-Smirnov hace que se rechace la hipótesis nula y se concluya que los datos no provienen de una distribución normal.

### popularity 
```{r, echo = FALSE}
qqnorm(datos$popularity)
qqline(datos$popularity)
lillie.test(datos$popularity)
```
El valor-p de el test Kolmogorov-Smirnov nos indica que se rechaza la hipótesis nula, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### voteAvg  
```{r, echo = FALSE}
qqnorm(datos$voteAvg)
qqline(datos$voteAvg)
lillie.test(datos$voteAvg)
```
El valor-p de el test Kolmogorov-Smirnov y la gráfica indican que los datos no respetan una distribución normal.

### voteCount
```{r, echo = FALSE}
qqnorm(datos$voteCount)
qqline(datos$voteCount)
lillie.test(datos$voteCount)
```
Nuevamente, el valor-p del test de Kolmogorov-Smirnov es menor a 0.05, por lo tanto, se rechaza la hipótesis nula y se argumenta que los datos no provienen de una distribución normal.

### genresAmount
```{r, echo = FALSE}
qqnorm(datos$genresAmount)
qqline(datos$genresAmount)
lillie.test(datos$genresAmount)
```
Con un valor-p menor a 0.05 en el test Kolmogorov-Smirnov se rechaza la hipótesis nula y se concluye que los datos no provienen de una distribución normal, argumento que refuerza la gráfica.

### productionCoAomunt
```{r, echo = FALSE}
qqnorm(datos$productionCoAmount)
qqline(datos$productionCoAmount)
lillie.test(datos$productionCoAmount)
```
El test Kolmogorov-Smirnov revela un valor-p menor a 0.05, además la gráfica nos muestra que los datos no se mantienen cercanos a la recta teórica en su totalidad, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### productionCountriesAmount 
```{r, echo = FALSE}
qqnorm(datos$productionCountriesAmount)
qqline(datos$productionCountriesAmount)
lillie.test(datos$productionCountriesAmount)
```
Al observar la gráfica podemos notar que los datos no se mantienen cercanos a la recta teórica en su totalidad, además el test Kolmogorov-Smirnov nos da un valor-p el cuál nos hace rechazar la hipótesis nula, argumentando así, que los datos no provienen de una distribución normal.

### actorsAmount 
```{r, echo = FALSE}
qqnorm(datos$actorsAmount)
qqline(datos$actorsAmount)
lillie.test(datos$actorsAmount)
```
Debido a que se obtiene un valor-p menor a 0.05 en el test de Kolmogorov-Smirnov, se rechaza la hipótesis nula y se concluye que los datos no provienen de una distribución normal.

### castWomenAmount 
```{r, echo = FALSE}
d1 <- as.numeric(datos$castWomenAmount)
qqnorm(d1)
qqline(d1)
lillie.test(d1)
```
Se puede notar en la gráfica que muchos de los datos no se mantienen en cercanía a la recta teórica, por lo tanto, se concluye que el set de datos no respeta una distribución normal, conclusión reforzada por el test Kolmogorov-Smirnov.

### castMenAmount 
```{r, echo = FALSE}
d2 <- as.numeric(datos$castMenAmount)
qqnorm(d2)
qqline(d2)
lillie.test(d2)
```
Nuevamente, obtenemos una gráfica donde los valores no se mantienen cercanos a la recta teórica y un valor-p menor a 0.05 en el test Kolmogorov-Smirnov, lo cuál hace que se concluya que no hay normalidad en el set de datos.


En el análisis exploratorio realizado se determinó que las variables cuantitativas no respetaban una distribución normal. Por lo tanto, todas las variables son a tomar en cuenta en el proceso.

```{r, echo=FALSE}

p <- as.numeric(datos[,"popularity"])
b <- as.numeric(datos[,"budget"])
r <- as.numeric(datos[,"revenue"])
r1 <- as.numeric(datos[,"runtime"])
g <- as.numeric(datos[,"genresAmount"])
p1 <- as.numeric(datos[,"productionCoAmount"])##
p2 <- as.numeric(datos[,"productionCountriesAmount"])
a <- as.numeric(datos[,"actorsAmount"])
c <- as.numeric(datos[,"castWomenAmount"])
c1 <- as.numeric(datos[,"castMenAmount"])
v <- as.numeric(datos[,"voteAvg"])
v1 <- as.numeric(datos[,"voteCount"])

datosc <- data.frame(p,b,r,r1,g,p1,p2,a,v,v1,c,c1)

#Escalar los datos
datosCS <- scale(na.omit(datosc))



```


## PROBLEMA 2

```{r} 
hopkins(datosCS)
#Matriz de distancia
datos_dist<- dist(datosCS)
```

Con un valor alejado a 0.5 se puede concluir que los datos no son aleatorios, por lo tanto, es adecuado el proceso de agrupamiento
```{r}
#knitr::opts_chunk$set(fig.width=12, fig.height=8) 
#fviz_dist(datos_dist, show_labels = F)
```

## PROBLEMA 3


```{r metodo de codo}
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(datosCS, centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
```


## PROBLEMA 4

K-means es un algoritmo de clasificación no supervisada (clusterización) que agrupa objetos en k grupos basándose en sus características. El agrupamiento se realiza minimizando la suma de distancias entre cada objeto y el centroide de su grupo o cluster. Se suele usar la distancia cuadrática.
 
```{r}
km<-kmeans(datosCS,5,iter.max =1000)
#datos$grupo<-km$cluster
plotcluster(datosCS,km$cluster) 
#plotcluster(datosCS[,1:12],km$cluster)
```

Como se observa en la imagen, el primer paso es escoger el numero de grupos K, en este caso fue 3 tal como se justificó anteriormente, posterior a ello se establecen k centroides en el espacio de datos.

```{r}
fviz_cluster(km, data = datosCS,geom = "point", ellipse.type = "norm")
```


Luego se asignan los centroides y se reubican.
```{r}
silkm<-silhouette(km$cluster,dist(datosCS))
mean(silkm[,3]) 
Kmean<-mean(silkm[,3]) 
```
Se obtiene que la silueta de K-means es de `r mean(silkm[,3])` esto indica que tenemos un buen resultado, ya que es muy cercano a 1, siendo un resultado deseable. </br>


#### Cluster jerarquico
El algortimo de clúster jerárquico agrupa los datos basándose en la distancia entre cada uno y buscando que los datos que están dentro de un clúster sean los más similares entre sí.

```{r}
matriz_dist<- dist(datosCS)
hc<-hclust(datos_dist, method = "ward.D2") #Genera el clustering jerarquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
cutree(hc, h = 5)
rect.hclust(hc,k=5)
groups<-cutree(hc,k=5) #corta el dendograma, determinando el grupo de cada fila
```


```{r}
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) 
Jerarquico<-mean(silhc[,3]) 
```

Como se observa la silueta del algoritmo cluster jerárquico fue de `r mean(silhc[,3])`, indicando que su valor fue muy cercano a 1, siendo un resultado deseable.</br>


## PROBLEMA 5
El gráfico de la silueta de K-means sería el siguiente:
```{r}
plot(silkm, cex.names=.4, col=1:3, border=NA)
```


Y el gráfico de la silueta de Jerargico sería: 
```{r}
plot(silhc, cex.names=.4, col=1:3, border = NA)
```

Al recopilar el promedio de los datos obtenemos:
```{r}
df <- data.frame(Algoritmo=c("K-mean", "Jerarquico"),
                Silueta=c(Kmean, Jerarquico))
df
```

Se puede observar que el cluster jerárquico fue el que obtuvo el mejor resultado en la prueba de silueta pero que el algorito Kmeans esta por debajo por 0.03 puntos lo que indica que sus resultados son muy parecidos. 

## PROBLEMA 6


```{r}
library(GGally)
ggpairs(datosc,columns=c(4,5,6,9))
```
Estos son algunos de las variables analizadas para ver su correlación, su frencuencia y su normalidad. En las variables de la gráfica se puede observar que los datos tienen una tendencia a agruparse al lado izquierdo, es decir, que hay una mayor de variables con un valor cercano a 0. Además, se puede observar en los gráficos de caja y bigotes la tendencia central de las
variables se encuentra en el lado izquierdo de la gráfica. De igual forma la correlacion entre algunas de las varibales es baja lo que indica que podriamos ignorarlas al momento de volver a hacer otro analisis. De igual forma en varios datos analisados en el problema 1 observamos que no todas las variables siguen una distribucion normal lo que indica que hay varios datos atipicos, estos igual podrian ser utilizados para poder eliminar varibles y que no afecten los datos. 
