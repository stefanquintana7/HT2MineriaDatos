---
title: "HT2"
author: "Stefan Quintana, Sofía Escobar, Wilfredo Gallegos"
date: "2/16/2023"
output: html_document
---

```{r, echo=FALSE}
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(dplyr)
library(ggplot2)
```

## PROBLEMA 1

```{r setup, include=TRUE, cache = FALSE}
datos<-read.csv("movies.csv")
set.seed(123)
datos<-datos[complete.cases(datos),]
summary(datos)
```

### budget 
```{r, echo = FALSE}
library(nortest)
qqnorm(datos$budget)
qqline(datos$budget)
lillie.test(datos$budget)
```
### revenue
```{r, echo = FALSE}
qqnorm(datos$budget)
qqline(datos$budget)
lillie.test(datos$revenue)
```
Basándose en la gráfica, donde se puede notar que los puntos se alejan de la recta teórica, podemos argumentar que los datos no respetan una distribución normal, lo cuál se confirma en el test Kolmogorov-Smirnov, donde se rechaza la hipótesis nula, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### runtime 
```{r, echo = FALSE}
qqnorm(datos$runtime)
qqline(datos$runtime)
lillie.test(datos$runtime)
```
Análogamente con el set de datos anterior, el valor p del test Kolmogorov-Smirnov hace que se rechace la hipótesis nula y se concluya que los datos no provienen de una distribución normal.

### popularity 
```{r, echo = FALSE}
qqnorm(datos$popularity)
qqline(datos$popularity)
lillie.test(datos$popularity)
```
El valor-p de el test Kolmogorov-Smirnov nos indica que se rechaza la hipótesis nula, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### voteAvg  
```{r, echo = FALSE}
qqnorm(datos$voteAvg)
qqline(datos$voteAvg)
lillie.test(datos$voteAvg)
```
El valor-p de el test Kolmogorov-Smirnov y la gráfica indican que los datos no respetan una distribución normal.

### voteCount
```{r, echo = FALSE}
qqnorm(datos$voteCount)
qqline(datos$voteCount)
lillie.test(datos$voteCount)
```
Nuevamente, el valor-p del test de Kolmogorov-Smirnov es menor a 0.05, por lo tanto, se rechaza la hipótesis nula y se argumenta que los datos no provienen de una distribución normal.

### genresAmount
```{r, echo = FALSE}
qqnorm(datos$genresAmount)
qqline(datos$genresAmount)
lillie.test(datos$genresAmount)
```
Con un valor-p menor a 0.05 en el test Kolmogorov-Smirnov se rechaza la hipótesis nula y se concluye que los datos no provienen de una distribución normal, argumento que refuerza la gráfica.

### productionCoAomunt
```{r, echo = FALSE}
qqnorm(datos$productionCoAmount)
qqline(datos$productionCoAmount)
lillie.test(datos$productionCoAmount)
```
El test Kolmogorov-Smirnov revela un valor-p menor a 0.05, además la gráfica nos muestra que los datos no se mantienen cercanos a la recta teórica en su totalidad, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### productionCountriesAmount 
```{r, echo = FALSE}
qqnorm(datos$productionCountriesAmount)
qqline(datos$productionCountriesAmount)
lillie.test(datos$productionCountriesAmount)
```
Al observar la gráfica podemos notar que los datos no se mantienen cercanos a la recta teórica en su totalidad, además el test Kolmogorov-Smirnov nos da un valor-p el cuál nos hace rechazar la hipótesis nula, argumentando así, que los datos no provienen de una distribución normal.

### actorsAmount 
```{r, echo = FALSE}
qqnorm(datos$actorsAmount)
qqline(datos$actorsAmount)
lillie.test(datos$actorsAmount)
```
Debido a que se obtiene un valor-p menor a 0.05 en el test de Kolmogorov-Smirnov, se rechaza la hipótesis nula y se concluye que los datos no provienen de una distribución normal.

### castWomenAmount 
```{r, echo = FALSE}
d1 <- as.numeric(datos$castWomenAmount)
qqnorm(d1)
qqline(d1)
lillie.test(d1)
```
Se puede notar en la gráfica que muchos de los datos no se mantienen en cercanía a la recta teórica, por lo tanto, se concluye que el set de datos no respeta una distribución normal, conclusión reforzada por el test Kolmogorov-Smirnov.

### castMenAmount 
```{r, echo = FALSE}
d2 <- as.numeric(datos$castMenAmount)
qqnorm(d2)
qqline(d2)
lillie.test(d2)
```
Nuevamente, obtenemos una gráfica donde los valores no se mantienen cercanos a la recta teórica y un valor-p menor a 0.05 en el test Kolmogorov-Smirnov, lo cuál hace que se concluya que no hay normalidad en el set de datos.


En el análisis exploratorio realizado se determinó que las variables cuantitativas no respetaban una distribución normal. Por lo tanto, todas las variables son a tomar en cuenta en el proceso.

```{r, echo=FALSE}

p <- as.numeric(datos[,"popularity"])
b <- as.numeric(datos[,"budget"])
r <- as.numeric(datos[,"revenue"])
r1 <- as.numeric(datos[,"runtime"])
g <- as.numeric(datos[,"genresAmount"])
p1 <- as.numeric(datos[,"productionCoAmount"])##
p2 <- as.numeric(datos[,"productionCountriesAmount"])
a <- as.numeric(datos[,"actorsAmount"])
c <- as.numeric(datos[,"castWomenAmount"])
c1 <- as.numeric(datos[,"castMenAmount"])
v <- as.numeric(datos[,"voteAvg"])
v1 <- as.numeric(datos[,"voteCount"])

datosc <- data.frame(p,b,r,r1,g,p1,p2,a,v,v1,c,c1)

#Escalar los datos
datosCS <- scale(na.omit(datosc))



```


## PROBLEMA 2

```{r} 
hopkins(datosCS)
#Matriz de distancia
datos_dist<- dist(datosCS)
```

Con un valor alejado a 0.5 se puede concluir que los datos no son aleatorios, por lo tanto, es adecuado el proceso de agrupamiento
```{r}
#knitr::opts_chunk$set(fig.width=12, fig.height=8) 
#fviz_dist(datos_dist, show_labels = F)
```

## PROBLEMA 3

Determine cuál es el número de grupos a formar más adecuado para los datos que está trabajando. 
Haga una gráfica de codo y explique la razón de la elección de la cantidad de clústeres con la que 
trabajará. 


```{r metodo de codo}
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(datosCS, centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
```
Podemos determinar la cantidad de grupos o clusters a través de un análisis para la gráfica generada por el método del codo. Podemos observar la particularidad del gráfico anterior que muestra un primer codo para la cantidad de grupos 3, y luego se genera otro segundo codo falso en la cantidad 6, se realizaron algoritmos como en el problema 4 para 6 grupos/clusters pero el agrupamiento no era uniforme, por lo que se decidió escoger agrupar los datos en 3 grupos, que es la cantidad de grupos con lo que se desarrollaron las siguientes preguntas.

## PROBLEMA 4

Utilice  los  algoritmos  k-medias  y  clustering  jerárquico  para  agrupar.  Compare  los  resultados 
generados por cada uno. 
 
### K-means

```{r}
km<-kmeans(datosCS,3,iter.max =1000)
#datos$grupo<-km$cluster
plotcluster(datosCS,km$cluster) 
#plotcluster(datosCS[,1:12],km$cluster)
```

Como se observa en el grafico generado para k-Means, el primer paso es escoger el numero de grupos K, en este caso fue 3 tal como se dejo claron en el problema anterior. A continuación se presenta otro gráfico para K-Means y poder observar de mejor manera las agrupaciones.

```{r}
fviz_cluster(km, data = datosCS,geom = "point", ellipse.type = "norm")
```


### Cluster jerarquico

A continuación se presenta el gráfico generado para el algoritmo de agrupación jerarquico. 

```{r}
matriz_dist<- dist(datosCS)
hc<-hclust(datos_dist, method = "ward.D2") #Genera el clustering jerarquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
cutree(hc, h = 3)
rect.hclust(hc,k=3)
groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
```

##### Comparación entre K-Means y Jerarquico

En ambos gráficos generados para el algoritmo K-Means podemos observar que estan bien definidos los 3 grupos y tienen una ¨forma¨ similar, donde probablemente uno de los 3 grupos es ligeramente mas pequeño que los otros 2. En cambio, utilizando el algoritmo jerarquico presenta uno de los 3 grupos con un tamaño abismalmente más grande, uno de tamaño regular-pequeño y uno demasiado pequeño. En conclusión el K-Means nos generó agrupaciones más equitativas mentras que el segundo algoritmo presenta desvalance entre el tamaño de los grupos.

## PROBLEMA 5
El gráfico de la silueta de K-means sería el siguiente:
```{r}

silkm<-silhouette(km$cluster,dist(datosCS))
plot(silkm, cex.names=.4, col=1:4, border=NA)
Kmean<-mean(silkm[,3]) 
```


Y el gráfico de la silueta de Jerargico sería: 
```{r}
silhc<-silhouette(groups,datos_dist)
plot(silhc, cex.names=.5, col=1:4, border = NA)
Jerarquico<-mean(silhc[,3]) 
```

Al recopilar el promedio de los datos obtenemos:
```{r}
df <- data.frame(Algoritmo=c("K-mean", "Jerarquico"),
                Silueta=c(Kmean, Jerarquico))
df
```

Se puede observar que el cluster jerárquico fue el que obtuvo el mejor resultado en la prueba de silueta pero que el algorito Kmeans es mayor que el Jerarguico por 0.2 puntos. Al observar los valores de las siluetas observamos que no es la mejor agrupacion pero que no es mala de igua forma. Tambien observamos que en los grupos de ambas siluetas hay elementos mal ubicados ya que hay valores negativos dentro de las siluetas.

## PROBLEMA 6


```{r}
library(GGally)
ggpairs(datosc,columns=c(4,5,6,9))
```
Estos son algunos de las variables analizadas para ver su correlación, su frencuencia y su normalidad. En las variables de la gráfica se puede observar que los datos tienen una tendencia a agruparse al lado izquierdo, es decir, que hay una mayor de variables con un valor cercano a 0. Además, se puede observar en los gráficos de caja y bigotes la tendencia central de las
variables se encuentra en el lado izquierdo de la gráfica. De igual forma la correlacion entre algunas de las varibales es baja lo que indica que podriamos ignorarlas al momento de volver a hacer otro analisis. De igual forma en varios datos analizados en el problema 1 observamos que no todas las variables siguen una distribucion normal lo que indica que hay varios datos atipicos, estos igual podrian ser utilizados para poder eliminar varibles y que no afecten los datos. 
